{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96b2c14",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries and Set Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not set. Set it before running agents:\")\n",
    "    print(\"   export OPENAI_API_KEY='sk-...'\")\n",
    "else:\n",
    "    print(\"✓ OpenAI API key detected\")\n",
    "\n",
    "# Import from project\n",
    "from agents import Agent, MockAgent\n",
    "from config import ExperimentConfig, AUCTION_SEALED_BID\n",
    "\n",
    "print(\"\\n✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a968b",
   "metadata": {},
   "source": [
    "## Section 2: Load Real Ethical Healthcare Vignettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real dataset\n",
    "df_vignettes = pd.read_csv('Ethical-Reasoning-in-Mental-Health.csv')\n",
    "\n",
    "print(f\"Loaded {len(df_vignettes)} ethical healthcare vignettes\")\n",
    "print(f\"\\nColumns: {list(df_vignettes.columns)}\")\n",
    "print(f\"\\nSubcategories: {df_vignettes['subcategory'].unique()[:5]}...\")\n",
    "\n",
    "# Sample 3-5 vignettes for MVP\n",
    "n_vignettes = 3\n",
    "sampled_indices = random.sample(range(len(df_vignettes)), n_vignettes)\n",
    "sample_vignettes = df_vignettes.iloc[sampled_indices].to_dict('records')\n",
    "\n",
    "print(f\"\\n✓ Selected {n_vignettes} vignettes for demo:\")\n",
    "for i, vig in enumerate(sample_vignettes, 1):\n",
    "    print(f\"  {i}. {vig['subcategory']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c82d3b",
   "metadata": {},
   "source": [
    "## Section 3: Initialize OpenAI Client and Create Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 20 agents with different communication styles\n",
    "n_agents = 20\n",
    "styles = ['assertive'] * 5 + ['timid'] * 5 + ['calibrated'] * 5 + ['neutral'] * 5\n",
    "random.shuffle(styles)\n",
    "\n",
    "# Check if we want to use real OpenAI or mock agents for testing\n",
    "use_real_openai = True  # Set to False for testing without API calls\n",
    "\n",
    "agents = []\n",
    "for i in range(n_agents):\n",
    "    agent_class = Agent if use_real_openai else MockAgent\n",
    "    agent = agent_class(\n",
    "        agent_id=f\"agent_{i:02d}\",\n",
    "        communication_style=styles[i],\n",
    "        budget=1.0\n",
    "    )\n",
    "    agents.append(agent)\n",
    "\n",
    "agent_styles_count = Counter([a.communication_style for a in agents])\n",
    "print(f\"✓ Created {n_agents} agents:\")\n",
    "for style, count in agent_styles_count.items():\n",
    "    print(f\"  - {count} {style} agents\")\n",
    "\n",
    "print(f\"\\nUsing: {'Real OpenAI API' if use_real_openai else 'Mock agents (no API calls)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b61fb",
   "metadata": {},
   "source": [
    "## Section 4: Implement Auction Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sealed_bid_auction(bids: Dict[str, float]) -> tuple:\n",
    "    \"\"\"\n",
    "    Run a sealed-bid (first-price) auction.\n",
    "    Winner pays their bid.\n",
    "    \"\"\"\n",
    "    if not bids:\n",
    "        return None, 0\n",
    "    winner_id = max(bids, key=bids.get)\n",
    "    amount_paid = bids[winner_id]\n",
    "    return winner_id, amount_paid\n",
    "\n",
    "def run_auction_round(vignette: Dict, agents: List, config: ExperimentConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run one complete auction round on a single vignette.\n",
    "    \n",
    "    Phases:\n",
    "    1. Private Assessment: Each agent independently assesses the vignette\n",
    "    2. Auction: Agents bid for proposer role\n",
    "    3. Proposal & Interventions: Winner proposes; others can critique for fee\n",
    "    4. Vote: Agents vote on final answer\n",
    "    5. Payoff: Distribute rewards and deduct costs\n",
    "    \"\"\"\n",
    "    \n",
    "    vignette_id = vignette.get('id', 'unknown')\n",
    "    round_results = {\n",
    "        'vignette_id': vignette_id,\n",
    "        'vignette_category': vignette.get('subcategory', ''),\n",
    "        'agents': defaultdict(dict),\n",
    "        'bids': {},\n",
    "        'proposer': None,\n",
    "        'proposal': None,\n",
    "        'interventions': defaultdict(list),\n",
    "        'votes': defaultdict(list),\n",
    "        'costs_by_agent': defaultdict(float),\n",
    "    }\n",
    "    \n",
    "    # PHASE 1: PRIVATE ASSESSMENT\n",
    "    print(f\"  Phase 1: Private Assessment\")\n",
    "    for agent in agents:\n",
    "        assessment = agent.assess(vignette)\n",
    "        round_results['agents'][agent.agent_id]['assessment'] = assessment\n",
    "    \n",
    "    # PHASE 2: SEALED-BID AUCTION\n",
    "    print(f\"  Phase 2: Sealed-Bid Auction\")\n",
    "    bids = {}\n",
    "    for agent in agents:\n",
    "        assessment = round_results['agents'][agent.agent_id]['assessment']\n",
    "        bid = agent.bid(vignette, assessment)\n",
    "        bids[agent.agent_id] = bid\n",
    "    \n",
    "    round_results['bids'] = bids\n",
    "    proposer_id, auction_cost = run_sealed_bid_auction(bids)\n",
    "    round_results['proposer'] = proposer_id\n",
    "    round_results['costs_by_agent'][proposer_id] += auction_cost\n",
    "    \n",
    "    print(f\"    Proposer: {proposer_id} (bid: ${auction_cost:.4f})\")\n",
    "    \n",
    "    # PHASE 3: PROPOSAL & INTERVENTIONS\n",
    "    print(f\"  Phase 3: Proposal & Optional Critiques\")\n",
    "    proposer_agent = next(a for a in agents if a.agent_id == proposer_id)\n",
    "    proposer_assessment = round_results['agents'][proposer_id]['assessment']\n",
    "    proposal_result = proposer_agent.propose(vignette, proposer_assessment)\n",
    "    round_results['proposal'] = proposal_result['proposal_text']\n",
    "    round_results['costs_by_agent'][proposer_id] += proposal_result['cost']\n",
    "    \n",
    "    print(f\"    Proposal: \\\"{proposal_result['proposal_text'][:100]}...\\\"\")\n",
    "    print(f\"    Proposal cost: ${proposal_result['cost']:.4f}\")\n",
    "    \n",
    "    # Non-proposers can intervene\n",
    "    for agent in agents:\n",
    "        if agent.agent_id != proposer_id:\n",
    "            assessment = round_results['agents'][agent.agent_id]['assessment']\n",
    "            intervention = agent.intervene(vignette, proposal_result['proposal_text'], assessment)\n",
    "            if intervention:\n",
    "                round_results['interventions'][agent.agent_id] = intervention\n",
    "                round_results['costs_by_agent'][agent.agent_id] += intervention['cost']\n",
    "    \n",
    "    n_interventions = len(round_results['interventions'])\n",
    "    print(f\"    Interventions: {n_interventions} agents critiqued\")\n",
    "    \n",
    "    # PHASE 4: VOTING\n",
    "    print(f\"  Phase 4: Final Vote\")\n",
    "    options = vignette.get('options', [])\n",
    "    if isinstance(options, str):\n",
    "        try:\n",
    "            options = json.loads(options)\n",
    "        except:\n",
    "            options = []\n",
    "    \n",
    "    votes = Counter()\n",
    "    for agent in agents:\n",
    "        vote = agent.vote(options)\n",
    "        votes[vote] += 1\n",
    "        round_results['votes'][agent.agent_id] = vote\n",
    "    \n",
    "    consensus_answer = votes.most_common(1)[0][0] if votes else \"No consensus\"\n",
    "    consensus_votes = votes[consensus_answer]\n",
    "    round_results['consensus_answer'] = consensus_answer\n",
    "    round_results['consensus_votes'] = consensus_votes\n",
    "    \n",
    "    expected_answer = vignette.get('expected_reasoning', '')\n",
    "    correctness = 1.0 if consensus_answer and expected_answer in consensus_answer else 0.5  # Partial credit\n",
    "    round_results['correctness'] = correctness\n",
    "    \n",
    "    print(f\"    Consensus: \\\"{consensus_answer[:50]}...\\\" ({consensus_votes}/{len(agents)} votes)\")\n",
    "    print(f\"    Correctness: {correctness:.1%}\")\n",
    "    \n",
    "    # PHASE 5: PAYOFF\n",
    "    print(f\"  Phase 5: Payoff Calculation\")\n",
    "    reward_amount = 1.0 if correctness >= 0.8 else 0.5 if correctness >= 0.5 else 0.0\n",
    "    round_results['reward_pool'] = reward_amount * len(agents)\n",
    "    \n",
    "    total_costs = sum(round_results['costs_by_agent'].values())\n",
    "    total_rewards = 0.0\n",
    "    \n",
    "    for agent in agents:\n",
    "        # Each agent gets: (reward_amount - their_cost)\n",
    "        agent_cost = round_results['costs_by_agent'][agent.agent_id]\n",
    "        agent_reward = reward_amount - agent_cost\n",
    "        round_results['agents'][agent.agent_id]['reward'] = agent_reward\n",
    "        total_rewards += agent_reward\n",
    "    \n",
    "    round_results['total_costs'] = total_costs\n",
    "    round_results['total_rewards'] = total_rewards\n",
    "    \n",
    "    print(f\"    Reward per agent: ${reward_amount:.4f}\")\n",
    "    print(f\"    Total costs: ${total_costs:.4f}\")\n",
    "    print(f\"    Total rewards: ${total_rewards:.4f}\")\n",
    "    \n",
    "    return round_results\n",
    "\n",
    "print(\"✓ Auction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873467a8",
   "metadata": {},
   "source": [
    "## Section 5: Run Multi-Agent Auction Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset agents for fresh vignettes\n",
    "for agent in agents:\n",
    "    agent.reset_for_new_vignette()\n",
    "\n",
    "print(f\"Running {n_vignettes} auction rounds with {n_agents} agents...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_round_results = []\n",
    "\n",
    "for round_num, vignette in enumerate(sample_vignettes, 1):\n",
    "    print(f\"\\nRound {round_num}: {vignette['subcategory']}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    round_result = run_auction_round(vignette, agents, AUCTION_SEALED_BID)\n",
    "    all_round_results.append(round_result)\n",
    "    \n",
    "    # Reset agents for next round\n",
    "    for agent in agents:\n",
    "        agent.reset_for_new_vignette()\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ Simulation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6fca5",
   "metadata": {},
   "source": [
    "## Section 6: Analyze Agent Behavior and Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb078d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "print(\"\\nSimulation Summary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_correctness = sum(r['correctness'] for r in all_round_results) / len(all_round_results)\n",
    "total_costs = sum(r['total_costs'] for r in all_round_results)\n",
    "total_rewards = sum(r['total_rewards'] for r in all_round_results)\n",
    "\n",
    "print(f\"Average Correctness:  {total_correctness:.1%}\")\n",
    "print(f\"Total Costs (all rounds):  ${total_costs:.4f}\")\n",
    "print(f\"Total Rewards (all rounds): ${total_rewards:.4f}\")\n",
    "print(f\"Efficiency (Correctness / Cost): {total_correctness / (total_costs + 0.001):.2f}\")\n",
    "print(f\"\\nKey Insight: Higher total rewards = more strategic, meaningful collaboration\")\n",
    "print(f\"            (not just cost reduction, but correctness maintenance)\")\n",
    "\n",
    "# Agent-level analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Agent-Level Performance by Communication Style\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "agent_stats = defaultdict(lambda: {'total_cost': 0.0, 'total_reward': 0.0, 'count': 0})\n",
    "\n",
    "for round_result in all_round_results:\n",
    "    for agent_id, agent_data in round_result['agents'].items():\n",
    "        # Find agent's style\n",
    "        agent_obj = next((a for a in agents if a.agent_id == agent_id), None)\n",
    "        if agent_obj:\n",
    "            style = agent_obj.communication_style\n",
    "            cost = round_result['costs_by_agent'][agent_id]\n",
    "            reward = agent_data.get('reward', 0.0)\n",
    "            \n",
    "            agent_stats[style]['total_cost'] += cost\n",
    "            agent_stats[style]['total_reward'] += reward\n",
    "            agent_stats[style]['count'] += 1\n",
    "\n",
    "for style in sorted(agent_stats.keys()):\n",
    "    stats = agent_stats[style]\n",
    "    avg_cost = stats['total_cost'] / stats['count'] if stats['count'] > 0 else 0\n",
    "    avg_reward = stats['total_reward'] / stats['count'] if stats['count'] > 0 else 0\n",
    "    print(f\"\\n{style.upper()}:\")\n",
    "    print(f\"  Avg Cost:     ${avg_cost:.4f}\")\n",
    "    print(f\"  Avg Reward:   ${avg_reward:.4f}\")\n",
    "    print(f\"  Net Benefit:  ${avg_reward - avg_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7798a53",
   "metadata": {},
   "source": [
    "## Section 7: Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ef2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting\n",
    "sns.set_style('whitegrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('LLM Auction Optimization Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Correctness per vignette\n",
    "correctness_by_round = [r['correctness'] for r in all_round_results]\n",
    "categories = [r['vignette_category'][:20] for r in all_round_results]\n",
    "\n",
    "axes[0, 0].bar(range(len(correctness_by_round)), correctness_by_round, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_xticks(range(len(categories)))\n",
    "axes[0, 0].set_xticklabels(categories, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylabel('Correctness')\n",
    "axes[0, 0].set_title('Correctness by Vignette')\n",
    "axes[0, 0].set_ylim([0, 1.1])\n",
    "axes[0, 0].axhline(y=total_correctness, color='red', linestyle='--', label='Avg')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Cost vs Reward per round\n",
    "costs_by_round = [r['total_costs'] for r in all_round_results]\n",
    "rewards_by_round = [r['total_rewards'] for r in all_round_results]\n",
    "\n",
    "x = np.arange(len(all_round_results))\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, costs_by_round, width, label='Total Costs', alpha=0.7)\n",
    "axes[0, 1].bar(x + width/2, rewards_by_round, width, label='Total Rewards', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Round')\n",
    "axes[0, 1].set_ylabel('Amount ($)')\n",
    "axes[0, 1].set_title('Costs vs Rewards by Round')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels([f'R{i+1}' for i in range(len(all_round_results))])\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Agent performance by communication style\n",
    "styles_list = sorted(agent_stats.keys())\n",
    "avg_costs = [agent_stats[s]['total_cost'] / agent_stats[s]['count'] for s in styles_list]\n",
    "avg_rewards = [agent_stats[s]['total_reward'] / agent_stats[s]['count'] for s in styles_list]\n",
    "\n",
    "x_styles = np.arange(len(styles_list))\n",
    "axes[1, 0].bar(x_styles - width/2, avg_costs, width, label='Avg Cost', alpha=0.7)\n",
    "axes[1, 0].bar(x_styles + width/2, avg_rewards, width, label='Avg Reward', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Communication Style')\n",
    "axes[1, 0].set_ylabel('Amount ($)')\n",
    "axes[1, 0].set_title('Agent Performance by Communication Style')\n",
    "axes[1, 0].set_xticks(x_styles)\n",
    "axes[1, 0].set_xticklabels(styles_list, rotation=45, ha='right')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 4: Voting distribution on last round\n",
    "last_votes = all_round_results[-1]['votes']\n",
    "vote_counts = Counter(last_votes.values())\n",
    "\n",
    "axes[1, 1].pie(vote_counts.values(), labels=[f\"{k[:30]}...\" for k in vote_counts.keys()], autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title(f'Vote Distribution (Final Round)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('auction_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✓ Results saved to auction_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51b0a0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This MVP demonstrates:\n",
    "\n",
    "1. **Real LLM Agents**: 20 OpenAI-powered agents with distinct communication styles deliberating in real-time\n",
    "2. **Ethical Reasoning**: Agents reason about real healthcare dilemmas from the Ethical Reasoning in Mental Health dataset\n",
    "3. **Auction Mechanism**: Sealed-bid auctions create incentives for meaningful contribution and strategic bidding\n",
    "4. **Total Rewards Metric**: Shows whether mechanism promotes genuine collaboration (high correctness + managed costs) vs silence\n",
    "5. **Scalability**: Framework ready for token-price sweeps and larger experiments (all 50 vignettes × 20 agents)\n",
    "\n",
    "**Next Steps**:\n",
    "- Run token price sweep (0.0001 to 0.01) to find optimal pricing\n",
    "- Scale to all 50 vignettes\n",
    "- Compare against baselines (free discussion, turn-taking)\n",
    "- Analyze which communication styles perform best under different prices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
